<!doctype html>
<html data-theme="light" lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="css/pico.min.css">
    <title>Matheus Gadelha</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
      .float-right {
        float: right;
        margin-left: 1rem;
        margin-bottom: 1rem;
        max-width: 300px;
      }

      .float-left {
        float: left;
        block: block;
        margin-right: 1rem;
        margin-bottom: 1rem;
        max-width: 150px;
      }

      article {
        overflow: hidden;
      }

      footer {
        clear: both;
      }

      @media (max-width: 768px) {
        .float-right {
          float: none;
          display: block;
          margin: 0 auto 1rem auto;
          max-width: 100%;
        }

        .float-left {
          float: none;
          display: block;
          margin: 0 auto 1rem auto;
          max-width: 100%;
        }
      }
    </style>
  </head>
  <body>

    <main class="container">
      <nav>
        <ul>
          <li></li>
        </ul>
        <ul>
          <li><a href="#info">Info</a></li>
          <li><a href="#papers">Papers</a></li>
          <li><a href="#service">Service</a></li>
          <li><a href="#experience">Experience</a></li>
        </ul>
      </nav>
      <h1 id="info">Matheus Gadelha</h1>

      <img src="fig/me.jpg" alt="Matheus Gadelha" class="float-right">
      <p>I am currently a Senior Research Scientist at <a href="https://research.adobe.com/">Adobe Research</a>.
I received my PhD from <a href="https://www.cics.umass.edu/">University of Massachusetts - Amherst</a>
while being supervised by <a href="https://people.cs.umass.edu/~ruiwang/">Prof. Rui Wang</a> and 
<a href="http://people.cs.umass.edu/~smaji/">Prof. Subhransu Maji</a>.
I am interested in Computer Graphics, Vision and their intersections with Machine Learning.
My work is focused on models and representations of tridimensional data for
both discriminative and generative models.</p>
<p>Lately, I have been particularly interested in mechanisms to incorporate 3D capabilities into
large generative models so we can properly control them and robustly create/understand
3D data. I am also broadly interested in techniques (not necessarily ML-based) that allow us
to better manipulate and author 3D content.</p>
<p><strong>Internships for PhD students:</strong> If you are interested in related areas to the ones I've mentioned above (or anything
related to my previous research), don't be shy and send me an e-mail with your CV and a short
description of the problems you are interested in working on. We are always looking for talented
interns to join us at Adobe Research.</p>
<p><strong>Pro-bono Mentoring:</strong> If you are a student/early-career researcher/enthusiast seeking advice on research projects
or general career directions, you can use <a href="https://outlook.office.com/book/Mentoring2@adobe.onmicrosoft.com/">this link</a> to schedule a chat with me.
I am particularly interested in chatting with folks from South/Latin America and helping in any way I can.</p>
<p><strong>Academic Collaborations:</strong> If you are a professor or a student interested in working with me,
feel free to send me an e-mail -- I am more than happy to discuss interesting research problems we
could work on together (or even just to chat about research).</p>
<p>[<a href="cv.pdf">CV</a>]
[<a href="https://bsky.app/profile/gadelha.bsky.social">bluesky</a>]
[<a href="https://www.linkedin.com/in/matheusgadelha/">linkedin</a>]
[<a href="mailto:matheusabrantesgadelha@gmail.com">email</a>]
[<a href="https://scholar.google.com/citations?user=VhqmvXsAAAAJ&amp;hl=en">scholar</a>]</p>

      <h2 id="techtransfer">Software</h2>
      <p><em>*I did not develop these by myself, but the list of collaborators is too big to include here.
They are the result of a big team effort.</em></p>
      <div class="grid">
        <div>
          <article>
<header>Adobe Illustrator</header>
<p><img class='float-left' alt="" src="fig/mockup_thumbnail.gif" /></p>
<p><strong>Mockup</strong></p>
<p>Users can apply their 2D designs into real photographs in a 3D-aware manner.</p>
<footer> &nbsp;&nbsp;&nbsp; <a href=https://helpx.adobe.com/illustrator/using/create-art-mockups.html><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>Adobe's Project Neo</header>
<p><img class='float-left' alt="" src="fig/Neo3D2Image.gif" /></p>
<p><strong>3D to Image</strong></p>
<p>Easily create 3D shapes using Project Neo and render them using a text-to-image generative model.</p>
<footer> &nbsp;&nbsp;&nbsp; <a href=https://projectneo.adobe.com/#><i class="fa-solid fa-globe"></i></a></footer>
</article>


        </div>
        <div>
          <article>
<header>Adobe Substance Viewer</header>
<p><img class='float-left' alt="" src="fig/SubstanceViewer3d2Image.gif" /></p>
<p><strong>3D to Image</strong></p>
<p>Using image generative models to "render" a 3D scene according to a text prompt
and style presets.</p>
<footer> &nbsp;&nbsp;&nbsp; <a href=https://helpx.adobe.com/substance-3d-viewer/using/gen-ai.html><i class="fa-solid fa-globe"></i></a></footer>
</article>


        </div>
      </div>

      <h2 id="papers">Papers</h2>
      <article>
<header>ArXiv</header>
<p><img class='float-left' alt="" src="fig/superfit_teaser.png" /></p>
<p><strong>Residual Primitive Fitting of 3D Shapes with SuperFrusta</strong></p>
<p><em>Aditya Ganeshan, <strong>Matheus Gadelha</strong>, Thibault Groueix, Zhiqin Chen, Siddhartha Chaudhuri, Vladimir Kim, Wang Yifan, Daniel Ritchie</em></p>
<footer><a href=https://arxiv.org/pdf/2512.09201><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://bardofcodes.github.io/superfit/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ICLR 2026</header>
<p><img class='float-left' alt="" src="fig/sigmagen.gif" /></p>
<p><strong>SIGMA-Gen: Structure and Identity Guided Multi-subject Assembly for Image Generation</strong></p>
<p><em>Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Kevin Blackburn-Matzen<code>*</code>, <strong>Matheus Gadelha</strong><code>*</code></em></p>
<footer><a href=https://arxiv.org/abs/2510.06469/><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://oindrilasaha.github.io/SIGMA-Gen/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>3DV 2026</header>
<p><img class='float-left' alt="" src="fig/stc_teaset.png" /></p>
<p><strong>Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal</strong></p>
<p><em>Rio Aguina-Kang, Kevin Blackburn-Matzen, Thibault Groueix, Vladimir Kim, <strong>Matheus Gadelha</strong></em></p>
<footer><a href=https://arxiv.org/abs/2602.04053><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; </footer>
</article>


<article>
<header>ArXiv</header>
<p><img class='float-left' alt="" src="fig/meshsplatting.gif" /></p>
<p><strong>MeshSplatting: Differentiable Rendering with Opaque Meshes</strong></p>
<p><em>Jan Held, Sanghyun Son, Renaud Vandeghen, Daniel Rebain, <strong>Matheus Gadelha</strong>, Yi Zhou, Anthony Cioppa, Ming C. Lin, Marc Van Droogenbroeck, Andrea Tagliasacchi</em></p>
<footer><a href=https://arxiv.org/abs/2512.06818><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://meshsplatting.github.io/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>NeurIPS 2025</header>
<p><img class='float-left' alt="" src="fig/innout.gif" /></p>
<p><strong>Frame In-N-Out: Unbounded Controllable Image-to-Video Generation</strong></p>
<p><em>Boyang Wang, Xuweiyi Chen, <strong>Matheus Gadelha</strong>, Zezhou Cheng</em></p>
<footer><a href=https://arxiv.org/abs/2505.21491><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://uva-computer-vision-lab.github.io/Frame-In-N-Out/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ICCV 2025</header>
<p><img class='float-left' alt="" src="fig/hierarchical_diffusion.png" /></p>
<p><strong>Reusing Computation in Text‑to‑Image Diffusion for Efficient Generation of Image Sets</strong></p>
<p><em>Dale Decatur, Thibault Groueix, Yifan Wang, Rana Hanocka, Vova Kim, <strong>Matheus Gadelha</strong></em></p>
<footer><a href=http://arxiv.org/pdf/2508.21032><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://ddecatur.github.io/hierarchical-diffusion/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ICCV 2025</header>
<p><img class='float-left' alt="" src="fig/dmeshpp.png" /></p>
<p><strong>DMesh++: An Efficient Differentiable Mesh for Complex Shapes</strong></p>
<p><em>Sanghyun Son, <strong>Matheus Gadelha</strong>, Yang Zhou, Matthew Fisher, Zexiang Xu, Ming C. Lin, Yi Zhou</em></p>
<footer><a href=https://arxiv.org/pdf/2412.16776><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://sonsang.github.io/dmesh2-project/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>SIGGRAPH 2025</header>
<p><img class='float-left' alt="" src="fig/teaser_3dfixup.gif" /></p>
<p><strong>3D-Fixup: Advancing Photo Editing with 3D Priors</strong></p>
<p><em>Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alex Schwing, Liangyan Gui, <strong>Matheus Gadelha</strong>, Paul Guerrero, Nanxuan Zhao</em></p>
<footer> &nbsp;&nbsp;&nbsp; <a href=https://3dfixup.github.io/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>CVPR 2025</header>
<p><img class='float-left' alt="" src="fig/precisecam.png" /></p>
<p><strong>PreciseCam: Precise Camera Control for Text-to-Image Generation</strong></p>
<p><em>Edurne Bernal-Berdun, Ana Serrano, Belen Masia, <strong>Matheus Gadelha</strong>, Yannick Hold-Geoffroy, Xin Sun, Diego Gutierrez</em></p>
<footer><a href=https://arxiv.org/pdf/2501.12910><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://graphics.unizar.es/projects/PreciseCam2024/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>CVPR 2025</header>
<p><img class='float-left' alt="" src="fig/motionmodes.gif" /></p>
<p><strong>Motion Modes: What Could Happen Next?</strong></p>
<p><em>Karran Pandey, <strong>Matheus Gadelha</strong>, Yannick Hold-Geoffroy, Karan Singh, Niloy J. Mitra, Paul Guerrero</em></p>
<footer><a href=https://motionmodes.github.io/resources/MotionModes.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://motionmodes.github.io/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>CVPR 2025</header>
<p><img class='float-left' alt="" src="fig/instant3dit.png" /></p>
<p><strong>Instant3dit: Multiview Inpainting for Fast Editing of 3D Objects</strong></p>
<p><em>Amir Barda, <strong>Matheus Gadelha</strong>, Vladimir Kim, Noam Aigerman, Amit Haim Bermano, Thibault Groueix</em></p>
<footer><a href=https://arxiv.org/pdf/2412.00518.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://amirbarda.github.io/Instant3dit.github.io/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>SIGGRAPH Asia 2024</header>
<p><img class='float-left' alt="" src="fig/mesh_refinement.png" /></p>
<p><strong>Text-guided Controllable Mesh Refinement for Interactive 3D Modeling</strong></p>
<p><em>Yun-Chun Chen, Selena Ling, Zhiqin Chen, Vladimir G. Kim, <strong>Matheus Gadelha</strong>, Alec Jacobson</em></p>
<footer><a href=https://arxiv.org/pdf/2406.01592><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://text-mesh-refinement-project.github.io/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>NeurIPS 2024</header>
<p><img class='float-left' alt="" src="fig/dmesh_teaser.gif" /></p>
<p><strong>DMesh: A Differentiable Representation for General Meshes</strong></p>
<p><em>Sanghyun Son, <strong>Matheus Gadelha</strong>, Yang Zhou, Zexiang Xu, Ming C. Lin, Yi Zhou</em></p>
<footer><a href=https://www.cs.umd.edu/~shh1295/dmesh/full2.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://sonsang.github.io/dmesh-project/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>SIGGRAPH 2024</header>
<p><img class='float-left' alt=" " src="fig/gem3d.gif" /></p>
<p><strong>GEM3D: Generative Medial Abstractions for 3D Shape Synthesis</strong></p>
<p><em>Dmitry Petrov, Pradyumn Goyal, Vikas Thamizharasan, Vova Kim, <strong>Matheus Gadelha</strong>, Melinos Averkiou, Siddhartha Chaudhuri, Evangelos Kalogerakis</em></p>
<footer><a href=https://arxiv.org/abs/2402.16994><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://lodurality.github.io/GEM3D/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>CVPR 2024</header>
<p><img class='float-left' alt=" " src="fig/c3dw.png" /></p>
<p><strong>Learning Continuous 3D Words for Text-to-Image Generation</strong></p>
<p><em>Ta-Ying Cheng, <strong>Matheus Gadelha</strong>, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, Niki Trigoni</em></p>
<footer><a href=https://ttchengab.github.io/continuous_3d_words/c3d_words.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://ttchengab.github.io/continuous_3d_words/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>CVPR 2024</header>
<p><img class='float-left' alt=" " src="fig/sunflower-gif.gif" /></p>
<p><strong>Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D</strong></p>
<p><em>Karran Pandey, Paul Guerrero, <strong>Matheus Gadelha</strong>, Yannick Hold-Geoffroy, Karan Singh, Niloy Mitra</em></p>
<footer><a href=https://arxiv.org/pdf/2312.02190.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://diffusionhandles.github.io/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>CVPR 2024</header>
<p><img class='float-left' alt=" " src="fig/genren.gif" /></p>
<p><strong>Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models</strong></p>
<p><em>Shengqu Cai, Duygu Ceylan, <strong>Matheus Gadelha</strong>, Chun-Hao Huang, Tuanfeng Y. Wang, Gordon Wetzstein</em></p>
<footer><a href=https://primecai.github.io/generative_rendering/assets/pdf/low_res.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://primecai.github.io/generative_rendering/index><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ICCV 2023</header>
<p><img class='float-left' alt=" " src="fig/3dminer.png" /></p>
<p><strong>3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets</strong></p>
<p><em>Ta-Ying Cheng, <strong>Matheus Gadelha</strong>, Soren Pirk, Thibault Groueix, Radomir Mech, Andrew Markham, Niki Trigoni</em></p>
<footer><a href=https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_3DMiner_Discovering_Shapes_from_Large-Scale_Unannotated_Image_Datasets_ICCV_2023_paper.html><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://ttchengab.github.io/3dminerOfficial/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>TVCG</header>
<p><img class='float-left' alt=" " src="fig/anise.png" /></p>
<p><strong>ANISE: Assembly-based Neural Implicit Surface rEconstruction</strong></p>
<p><em>Dmitry Petrov, <strong>Matheus Gadelha</strong>, Radomir Mech, Evangelos Kalogerakis</em></p>
<footer><a href=https://arxiv.org/pdf/2205.13682.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://lodurality.github.io/ANISE/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ICCV 2023 Workshop</header>
<p><img class='float-left' alt=" " src="fig/turntables.png" /></p>
<p><strong>Accidental Turntables: Learning 3D Pose by Watching Objects Turn</strong></p>
<p><em>Zezhou Cheng, <strong>Matheus Gadelha</strong>, Subhransu Maji</em></p>
<footer><a href=https://arxiv.org/pdf/2212.06300.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://people.cs.umass.edu/~zezhoucheng/acci-turn/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ECCV 2022 Workshop</header>
<p><img class='float-left' alt=" " src="fig/recovdetail.png" /></p>
<p><strong>Recovering Detail in 3D Shapes Using Disparity Maps</strong></p>
<p><em>Marissa Ramirez de Chanlatte, <strong>Matheus Gadelha</strong>, Thibault Groueix, Radomir Mech</em></p>
<footer><a href=https://arxiv.org/pdf/2207.00182.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; </footer>
</article>


<article>
<header>SGP 2022</header>
<p><img class='float-left' alt=" " src="fig/surfit.png" /></p>
<p><strong>PrimFit: Learning to Fit Primitives Improves Few Shot Learning on Point Clouds</strong></p>
<p><em>Gopal Sharma, Bidya Dash, <strong>Matheus Gadelha</strong>, Aruni RoyChowdhury, Marios Loizou, Evangelos Kalogerakis, Liangliang Cao, Erik Learned-Miller, Rui Wang and Subhransu Maji</em></p>
<footer><a href=https://arxiv.org/pdf/2112.13942.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; </footer>
</article>


<article>
<header>CVPR 2022</header>
<p><img class='float-left' alt=" " src="fig/planar_recon.png" /></p>
<p><strong>PlanarRecon: Real-time 3D Plane Detection and Reconstruction from Posed Monocular Videos</strong></p>
<p><em>Yiming Xie, <strong>Matheus Gadelha</strong>, Fengting Yang, Xiaowei Zhou, Huaizu Jiang</em></p>
<footer><a href=https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_PlanarRecon_Real-Time_3D_Plane_Detection_and_Reconstruction_From_Posed_Monocular_CVPR_2022_paper.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://neu-vi.github.io/planarrecon/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>IEEE AIVR 2021</header>
<p><img class='float-left' alt=" " src="fig/tmm_paper.png" /></p>
<p><strong>Trace Match &amp; Merge: Long-TermField-Of-View Prediction for AR Applications</strong></p>
<p><em>Adam Viola<code>*</code>, Sahil Sharma<code>*</code>,Pankaj Bishnoi<code>*</code>, <strong>Matheus Gadelha</strong>, Stefano Petrangeli, Haoliang Wang, Viswanathan Swaminathan</em></p>
<p><strong>Best paper candidate at <a href="https://ieee-aivr.cs.nthu.edu.tw/program">IEEE AIVR</a></strong>.</p>
<footer><a href=AIVR_2021___AR_FOV_prediction.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; </footer>
</article>


<article>
<header>IJCV 2020</header>
<p><img class='float-left' alt=" " src="fig/prgan_journal.png" /></p>
<p><strong>Inferring 3D Shapes from Image Collections using Adversarial Networks</strong></p>
<p><em><strong>Matheus Gadelha</strong>, Aartika Rai, Rui Wang, Subhransu Maji</em></p>
<footer><a href=https://arxiv.org/abs/1906.04910><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; </footer>
</article>


<article>
<header>ECCV 2020</header>
<p><img class='float-left' alt=" " src="fig/acd.png" /></p>
<p><strong>Label-Efficient Learning on Point Clouds using Approximate Convex Decompositions</strong></p>
<p><em><strong>Matheus Gadelha</strong><code>*</code>, Aruni RoyChowdhury<code>*</code>, Gopal Sharma, Evangelos Kalogerakis, Liangliang Cao, Erik Learned-Miller, Rui Wang, Subhransu Maji</em></p>
<footer><a href=https://arxiv.org/pdf/2003.13834.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=selfsupacd/index.html><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>arXiv</header>
<p><img class='float-left' alt=" " src="fig/dmp_snapshot.png" /></p>
<p><strong>Deep Manifold Prior</strong></p>
<p><em><strong>Matheus Gadelha</strong>, Rui Wang, Subhransu Maji</em></p>
<p><strong>Best poster honorable mention at <a href="http://visual.cs.brown.edu/workshops/necv2019/">NECV</a></strong>.</p>
<footer><a href=https://arxiv.org/pdf/2004.04242.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; </footer>
</article>


<article>
<header>CVPR 2020</header>
<p><img class='float-left' alt=" " src="shapehandles/fig/snapshot.png" /></p>
<p><strong>Learning Generative Models of Shape Handles</strong></p>
<p><em><strong>Matheus Gadelha</strong>, Giorgio Gori, Duygu Ceylan, Radomir Mech, Nathan Carr, Tamy Boubekeur, Rui Wang, Subhransu Maji</em></p>
<footer> &nbsp;&nbsp;&nbsp; <a href=shapehandles/index.html><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ICCV 2019</header>
<p><img class='float-left' alt=" " src="shaperec/fig/abstract.png" /></p>
<p><strong>Shape Reconstruction with Differentiable Projections and Deep Priors</strong></p>
<p><em><strong>Matheus Gadelha</strong>, Rui Wang, Subhransu Maji</em></p>
<footer><a href=shaperec/dsp_paper.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=shaperec/index.html><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>CVPR 2019</header>
<p><img class='float-left' alt=" " src="fig/bayesdip.png" /></p>
<p><strong>A Bayesian Perspective on the Deep Image Prior</strong></p>
<p><em>Zezhou Cheng, <strong>Matheus Gadelha</strong>, Subhransu Maji, Daniel Sheldon</em></p>
<footer><a href=https://arxiv.org/abs/1904.07457><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://people.cs.umass.edu/~zezhoucheng/gp-dip/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ECCV 2018 - 3DRMS Workshop</header>
<p><img class='float-left' alt=" " src="fig/3dsc.png" /></p>
<p><strong>A Deeper Look at 3D Shape Classifiers</strong></p>
<p><em>Jong-Chyi Su, <strong>Matheus Gadelha</strong>, Rui Wang, Subhransu Maji</em></p>
<p></br></p>
<footer><a href=https://arxiv.org/abs/1809.02560><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://people.cs.umass.edu/~jcsu/papers/shape_recog/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>ECCV 2018</header>
<p><img class='float-left' alt=" " src="mrt/fig/smallabs.png" /></p>
<p><strong>Multiresolution Tree Networks for Point Cloud Processing</strong></p>
<p><em><strong>Matheus Gadelha</strong>, Rui Wang, Subhransu Maji</em></p>
<footer><a href=https://arxiv.org/abs/1807.03520><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=mrt/index.html><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>3DV 2017</header>
<p><img class='float-left' alt=" " src="prgan/fig/overview.png" /></p>
<p><strong>Unsupervised 3D Shape Induction from 2D Views of Multiple Objects</strong></p>
<p><em><strong>Matheus Gadelha</strong>, Subhransu Maji, Rui Wang</em></p>
<footer><a href=https://arxiv.org/pdf/1612.05872.pdf><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=prgan/index.html><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>3DV 2017</header>
<p><img class='float-left' alt=" " src="fig/sketch.png" /></p>
<p><strong>3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks</strong></p>
<p><em>Zhaoliang Lun, <strong>Matheus Gadelha</strong>, Evangelos Kalogerakis, Subhransu Maji, Rui Wang</em></p>
<footer><a href=https://arxiv.org/abs/1707.06375><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=https://people.cs.umass.edu/~zlun/papers/SketchModeling/><i class="fa-solid fa-globe"></i></a></footer>
</article>


<article>
<header>BMVC 2017</header>
<p><img class='float-left' alt=" " src="sppc/fig/airplanes_sorting_new2.png" /></p>
<p><strong>Shape Generation using Spatially Partitioned Point Clouds</strong></p>
<p><em><strong>Matheus Gadelha</strong>, Subhransu Maji, Rui Wang</em></p>
<footer><a href=https://arxiv.org/abs/1707.06267><i class="fa-solid fa-file-pdf"></i></a> &nbsp;&nbsp;&nbsp; <a href=sppc/index.html><i class="fa-solid fa-globe"></i></a></footer>
</article>



      <h2 id="service">Service</h2>
      <p>I worked/am working as a reviewer for the following venues:</p>
<ul>
<li>SIGGRAPH 2023-2025</li>
<li>CVPR 2018-2024 (<strong>2021 outstanding reviewer</strong>)</li>
<li>ICCV 2019-2023</li>
<li>ECCV 2018-2023</li>
<li>ICLR 2021</li>
<li>TPAMI (2018, 2021, 2023) </li>
<li>IJCV 2023, 2024</li>
<li>Computer and Graphics 2018, 2024</li>
<li>SIGGRAPH Asia 2018, 2024</li>
<li>Pacific Graphics 2019</li>
</ul>
<p>I worked/am working as AC for the following venues:</p>
<ul>
<li>WACV 2024, 2025</li>
<li>CVPR 2025, 2026</li>
</ul>

      <h2 id="experience">Professional Experience</h2>
      <article><p><img class='float-left' alt="" src="fig/adobe_logo.png" />
<strong>Adobe</strong>.</p>
<p>Research Scientist (Seattle, WA).</p>
<p><em>June 2021 - Now</em>.</p></article>
<article><p><img class='float-left' alt="" src="fig/google_logo.png" />
<strong>Google (Perception)</strong>. </p>
<p>Research Intern (Amherst, MA). </p>
<p><em>Summer 2020</em>.</p></article>
<article><p><img class='float-left' alt="" src="fig/adobe_logo.png" />
<strong>Adobe</strong>.</p>
<p>Research Intern (San Jose, CA)</p>
<p><em>Summer 2019</em>.</p></article>
<article><p><img class='float-left' alt="" src="fig/amazon_logo.png" />
<strong>Amazon</strong>. </p>
<p>Applied Scientist Intern (Pasadena, CA)</p>
<p><em>Summer 2018</em>.</p></article>
<article><p><img class='float-left' alt="" src="fig/brasao_ufrn.png" />
<strong>Federal University at Rio Grande do Norte</strong>. </p>
<p>Temporary Letcturer - Introduction to Algorithms and Numerical Methods. </p>
<p><em>June 2014 - June2015 </em>.</p></article>

    </main>
  </body>
</html>
